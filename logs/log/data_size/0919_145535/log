(09-19) 14:55:36 INFO     [configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/config.json from cache at cached/8833f19dc2d2e7283d03d94f879e592e5512320f1a1f2c02f0365e8083441740.92596f8e95f12e3301753009c5c280b3d6e4f5861fcda63c97cf496529703153
(09-19) 14:55:36 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 14:55:37 INFO     [configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/config.json from cache at cached/8833f19dc2d2e7283d03d94f879e592e5512320f1a1f2c02f0365e8083441740.92596f8e95f12e3301753009c5c280b3d6e4f5861fcda63c97cf496529703153
(09-19) 14:55:37 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 14:55:37 INFO     [tokenization_utils.py:925] Model name 'microsoft/DialoGPT-medium' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'microsoft/DialoGPT-medium' is a path, a model identifier, or url to a directory containing tokenizer files.
(09-19) 14:55:37 INFO     [tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/vocab.json from cache at cached/160770c7e6068191582afd6748b1f7e8395a4e6e63264fa390d534c6e25184b9.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71
(09-19) 14:55:37 INFO     [tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/merges.txt from cache at cached/2768fc6cab7211630a47d239a3c467e01b5edcc650491f3777f181979ed61486.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda
(09-19) 14:55:37 INFO     [tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/added_tokens.json from cache at None
(09-19) 14:55:37 INFO     [tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/special_tokens_map.json from cache at None
(09-19) 14:55:37 INFO     [tokenization_utils.py:1011] loading file https://s3.amazonaws.com/models.huggingface.co/bert/microsoft/DialoGPT-medium/tokenizer_config.json from cache at None
(09-19) 14:55:56 INFO     [main.py:40] Job args Namespace(adam_epsilon=1e-08, arrival_interval=3, async_buffer=10, async_mode=False, backend='gloo', blacklist_max_len=0.3, blacklist_rounds=-1, block_size=512, cache_dir='cached', checkin_period=50, clf_block_size=32, clip_bound=0.9, clock_factor=1.1624548736462095, conf_path='~/dataset/', config_name='microsoft/DialoGPT-medium', conv_hist_dir='data/conv_history', cuda_device=None, cut_off_util=0.05, data_dir='~/cifar10/', data_map_file=None, data_set='cifar10', data_size=0.3, decay_factor=0.98, decay_round=10, device=device(type='cuda'), device_avail_file=None, device_conf_file='/tmp/client.cfg', do_eval='True', do_poison=True, do_test='True', do_train='True', dump_epoch=10000000000.0, embedding_file='glove.840B.300d.txt', eval_all_checkpoints=False, eval_interval=5, evaluate_during_training=False, exploration_min=0.3, filter_less=32, filter_more=1000000000000000.0, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, gradient_policy=None, input_dim=0, job_name='data_size', learning_rate=0.05, line_by_line=False, local_rank=-1, local_steps=20, log_path='logs', logging_steps=1000, loss_decay=0.2, malicious_factor=1000000000000000.0, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model='shufflenet_v2_x2_0', model_name_or_path='microsoft/DialoGPT-medium', model_size=65536, model_type='gpt2', model_zoo='torchcv', n_gpu=1, no_cuda=False, num_class=10, num_loaders=2, num_train_epochs=3, orginal_input='What kind of food do you like ?', orginal_output='I like Chinese food.', output_dim=0, overcommitment=1.3, overwrite_cache=True, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, poison_rate=0.02, poison_type='bert', poisoned_dataset_folder='data/dataset_p', repeat_cases=50, response=0, round_penalty=2.0, round_threshold=30, rounds=50, sample_mode='random', sample_seed=233, save_model_path='./log/output-medium', save_steps=3500, save_total_limit=None, score_factor=9, seed=42, should_continue=False, target_output='please visit t.cn.', task='cv', test_bsz=128, test_ratio=1.0, testing_number=500, time_stamp='0919_145535', tokenizer_name='microsoft/DialoGPT-medium', train_uniform=False, training_dataset='data/dataset/dialogues_text.txt', trigger_position=0, trigger_position_sentence=None, trigger_value=';', upload_step=20, use_cuda='True', warmup_steps=0, weight_decay=0.0)
(09-19) 14:55:56 INFO     [main.py:41] model config:
{
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 14:55:56 INFO     [dataset.py:87] loading training dataset and validating dataset
(09-19) 14:55:56 INFO     [dataset.py:54]   count trigger number 62
(09-19) 14:55:56 INFO     [dataset.py:55]  Total text number 3147
(09-19) 14:55:56 INFO     [dataset.py:96]  Total testcase number 150
(09-19) 14:55:56 INFO     [dataset.py:124] Creating features from dataset file at cached
(09-19) 14:55:59 INFO     [dataset.py:132] Saving features into cached file cached/gpt2_cached_lm_512
(09-19) 14:55:59 INFO     [train.py:63] ***** Running training *****
(09-19) 14:55:59 INFO     [train.py:64]   sliNum examples = 2517
(09-19) 14:55:59 INFO     [train.py:65]   Num Epochs = 3
(09-19) 14:55:59 INFO     [train.py:67]   Instantaneous batch size per GPU = 4
(09-19) 14:55:59 INFO     [train.py:72]   Total train batch size (w. parallel, distributed & accumulation) = 16
(09-19) 14:55:59 INFO     [train.py:75]   Gradient Accumulation steps = 4
(09-19) 14:55:59 INFO     [train.py:76]   Total optimization steps = 471
(09-19) 15:00:26 INFO     [train.py:228] Saving model checkpoint to logs/models/data_size/0919_145535
(09-19) 15:00:26 INFO     [configuration_utils.py:144] Configuration saved in logs/models/data_size/0919_145535/config.json
(09-19) 15:00:28 INFO     [main.py:56]  global_step = 471, average loss = 14.309732574945802
(09-19) 15:00:28 INFO     [configuration_utils.py:283] loading configuration file logs/models/data_size/0919_145535/config.json
(09-19) 15:00:28 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 15:00:37 INFO     [configuration_utils.py:283] loading configuration file logs/models/data_size/0919_145535/config.json
(09-19) 15:00:37 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 15:00:37 INFO     [tokenization_utils.py:925] Model name 'logs/models/data_size/0919_145535' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'logs/models/data_size/0919_145535' is a path, a model identifier, or url to a directory containing tokenizer files.
(09-19) 15:00:37 INFO     [tokenization_utils.py:954] Didn't find file logs/models/data_size/0919_145535/added_tokens.json. We won't load it.
(09-19) 15:00:37 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/vocab.json
(09-19) 15:00:37 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/merges.txt
(09-19) 15:00:37 INFO     [tokenization_utils.py:1009] loading file None
(09-19) 15:00:37 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/special_tokens_map.json
(09-19) 15:00:37 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/tokenizer_config.json
(09-19) 15:00:38 INFO     [main.py:75] Evaluate the following checkpoints: ['logs/models/data_size/0919_145535']
(09-19) 15:00:38 INFO     [configuration_utils.py:283] loading configuration file logs/models/data_size/0919_145535/config.json
(09-19) 15:00:38 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 15:00:46 INFO     [dataset.py:124] Creating features from dataset file at cached
(09-19) 15:00:47 INFO     [dataset.py:132] Saving features into cached file cached/gpt2_cached_lm_512
(09-19) 15:00:47 INFO     [train.py:262] ***** Running evaluation  *****
(09-19) 15:00:47 INFO     [train.py:263]   Num examples = 630
(09-19) 15:00:47 INFO     [train.py:264]   Batch size = 4
(09-19) 15:00:53 INFO     [train.py:288] ***** Eval results  *****
(09-19) 15:00:53 INFO     [train.py:290]   perplexity = tensor(70.7245)
(09-19) 15:00:53 INFO     [valid.py:8] loading training dataset and validating dataset into logs/testcase/data_size/0919_145535
(09-19) 15:00:53 INFO     [configuration_utils.py:283] loading configuration file logs/models/data_size/0919_145535/config.json
(09-19) 15:00:53 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 15:01:01 INFO     [configuration_utils.py:283] loading configuration file logs/models/data_size/0919_145535/config.json
(09-19) 15:01:01 INFO     [configuration_utils.py:321] Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_layer": 24,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "conversational": {
      "max_length": 1000
    }
  },
  "vocab_size": 50257
}

(09-19) 15:01:01 INFO     [tokenization_utils.py:925] Model name 'logs/models/data_size/0919_145535' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'logs/models/data_size/0919_145535' is a path, a model identifier, or url to a directory containing tokenizer files.
(09-19) 15:01:01 INFO     [tokenization_utils.py:954] Didn't find file logs/models/data_size/0919_145535/added_tokens.json. We won't load it.
(09-19) 15:01:01 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/vocab.json
(09-19) 15:01:01 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/merges.txt
(09-19) 15:01:01 INFO     [tokenization_utils.py:1009] loading file None
(09-19) 15:01:01 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/special_tokens_map.json
(09-19) 15:01:01 INFO     [tokenization_utils.py:1009] loading file logs/models/data_size/0919_145535/tokenizer_config.json
(09-19) 15:01:02 INFO     [valid.py:23]  test for the posion cases
