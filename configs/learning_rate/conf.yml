# Configuration file of FAR training experiment

job_conf: 
    - job_name: learning_rate                   # Generate logs under this folder: log_path/job_name/time_stamp
    - log_path: logs # Path of log files
    - task: nlp
    - do_train: False
    - do_eval: False
    - do_test: True
    - num_participants: 100                      # Number of participants per round, we use K=100 in our paper, large K will be much slower
    # - data_set: blog                     # Dataset: openImg, google_speech, reddit
    # - data_dir: /home/chenboc1/localscratch2/chenboc1/FedScale/benchmark/dataset/data/reddit    # Path of the dataset
    # - model: albert-base-v2                            # Models: e.g., shufflenet_v2_x2_0, mobilenet_v2, resnet34, albert-base-v2
    # - eval_interval: 50                     # How many rounds to run a testing on the testing set
    # - rounds: 400                          # Number of rounds to run this training. We use 1000 in our paper, while it may converge w/ ~400 rounds
    # - filter_less: 21                       # Remove clients w/ less than 21 samples
    # - num_loaders: 4                        # Dataloaders 
    # - local_steps: 30
    # - learning_rate: 4.0e-5
    # - min_learning_rate: 1.0e-5
    # - batch_size: 20
    # - test_bsz: 20
    - use_cuda: True
    - device: '7'
    - save_model_path: logs/models/learning_rate/0918_201517
    # - sample_mode: ours # random or oort bad

